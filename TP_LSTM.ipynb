{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6HpUT4aMX5gL4BLGZf879",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AArna1211/Traffic-prediction/blob/main/TP_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IYeBNYI09HdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgsJkESx9CKl",
        "outputId": "788dbd11-8a49-4398-bab4-e2a53e023fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-c6e80437b0e7>:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  data[\"Timestamp\"] = pd.to_datetime(data[\"Time\"] + \" \" + data[\"Date\"].astype(str))\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - accuracy: 0.6269 - loss: 0.9851 - val_accuracy: 0.7378 - val_loss: 0.7753\n",
            "Epoch 2/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7262 - loss: 0.7805 - val_accuracy: 0.7261 - val_loss: 0.7771\n",
            "Epoch 3/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7104 - loss: 0.7941 - val_accuracy: 0.7361 - val_loss: 0.7560\n",
            "Epoch 4/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7377 - loss: 0.7431 - val_accuracy: 0.7429 - val_loss: 0.7469\n",
            "Epoch 5/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7294 - loss: 0.7594 - val_accuracy: 0.7378 - val_loss: 0.7358\n",
            "Epoch 6/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7320 - loss: 0.7503 - val_accuracy: 0.7462 - val_loss: 0.7448\n",
            "Epoch 7/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7421 - loss: 0.7297 - val_accuracy: 0.7395 - val_loss: 0.7597\n",
            "Epoch 8/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7193 - loss: 0.7572 - val_accuracy: 0.7395 - val_loss: 0.7221\n",
            "Epoch 9/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7384 - loss: 0.7475 - val_accuracy: 0.7395 - val_loss: 0.7430\n",
            "Epoch 10/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7303 - loss: 0.7330 - val_accuracy: 0.7143 - val_loss: 0.7966\n",
            "Epoch 11/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7344 - loss: 0.7310 - val_accuracy: 0.7345 - val_loss: 0.7280\n",
            "Epoch 12/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7371 - loss: 0.7474 - val_accuracy: 0.7429 - val_loss: 0.7299\n",
            "Epoch 13/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.7280 - loss: 0.7460 - val_accuracy: 0.7513 - val_loss: 0.7227\n",
            "Epoch 14/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7403 - loss: 0.7267 - val_accuracy: 0.7496 - val_loss: 0.7262\n",
            "Epoch 15/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7333 - loss: 0.7398 - val_accuracy: 0.7412 - val_loss: 0.7227\n",
            "Epoch 16/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7326 - loss: 0.7268 - val_accuracy: 0.7429 - val_loss: 0.7046\n",
            "Epoch 17/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7154 - loss: 0.7598 - val_accuracy: 0.7479 - val_loss: 0.7209\n",
            "Epoch 18/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7468 - loss: 0.7198 - val_accuracy: 0.7395 - val_loss: 0.7378\n",
            "Epoch 19/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7330 - loss: 0.7226 - val_accuracy: 0.7479 - val_loss: 0.7071\n",
            "Epoch 20/20\n",
            "\u001b[1m2379/2379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7513 - loss: 0.7021 - val_accuracy: 0.7496 - val_loss: 0.7226\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Test Accuracy: 0.7496\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "data = pd.read_csv('/content/Traffic.csv')\n",
        "\n",
        "# Convert Time & Date columns to datetime format\n",
        "data[\"Timestamp\"] = pd.to_datetime(data[\"Time\"] + \" \" + data[\"Date\"].astype(str))\n",
        "data = data.sort_values(by=\"Timestamp\")\n",
        "\n",
        "# Encode categorical \"Traffic Situation\"\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Traffic Situation Encoded\"] = label_encoder.fit_transform(data[\"Traffic Situation\"])\n",
        "\n",
        "# Select features for LSTM\n",
        "features = [\"CarCount\", \"BikeCount\", \"BusCount\", \"TruckCount\", \"Total\"]\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data[features])\n",
        "\n",
        "# Create time-series sequences\n",
        "def create_sequences(data, labels, seq_length=2):  # Adjusted for small dataset\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(labels[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 2\n",
        "X, y = create_sequences(data_scaled, data[\"Traffic Situation Encoded\"].values, seq_length)\n",
        "X = X.reshape(X.shape[0], X.shape[1], len(features))  # Reshape for LSTM\n",
        "\n",
        "# Split into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(seq_length, len(features))),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50),\n",
        "    Dropout(0.2),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "epochs = 20\n",
        "batch_size = 1  # Adjusted for small dataset\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Decode labels\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
        "y_test_labels = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = np.mean(y_pred_classes == y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    }
  ]
}